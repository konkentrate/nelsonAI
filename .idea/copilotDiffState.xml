<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/bot/agent.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/bot/agent.py" />
              <option name="originalContent" value="import os&#10;import json&#10;import discord&#10;from discord.ext import commands&#10;import httpx&#10;import time&#10;import asyncio&#10;from dotenv import load_dotenv&#10;from memory import ConversationMemory&#10;import sqlite3&#10;&#10;load_dotenv()&#10;&#10;DISCORD_TOKEN = os.getenv(&quot;DISCORD_TOKEN&quot;)&#10;MISTRAL_API_KEY = os.getenv(&quot;MISTRAL_API_KEY&quot;)&#10;ALLOWED_CHANNEL_ID = int(os.getenv(&quot;ALLOWED_CHANNEL_ID&quot;))&#10;&#10;# Define the agent ID to use or fallback to model name if not specified&#10;MISTRAL_AGENT_ID = os.getenv(&quot;MISTRAL_AGENT_ID&quot;)&#10;MISTRAL_MODEL = os.getenv(&quot;MISTRAL_MODEL&quot;, &quot;mistral-small&quot;)&#10;&#10;intents = discord.Intents.default()&#10;intents.message_content = True&#10;&#10;bot = commands.Bot(command_prefix=&quot;/&quot;, intents=intents)&#10;&#10;# Dictionary to store conversation IDs for each user&#10;user_conversations = {}&#10;&#10;# Initialize memory system for stats only&#10;memory = ConversationMemory()&#10;print(&quot;[DEBUG] Memory system initialized (used for stats only)&quot;)&#10;&#10;async def create_conversation():&#10;    &quot;&quot;&quot;Create a new conversation with Mistral Conversations API&quot;&quot;&quot;&#10;    print(&quot;[DEBUG] Creating new Mistral conversation&quot;)&#10;&#10;    headers = {&#10;        &quot;Authorization&quot;: f&quot;Bearer {MISTRAL_API_KEY}&quot;,&#10;        &quot;Content-Type&quot;: &quot;application/json&quot;&#10;    }&#10;&#10;    # Create payload with either agent_id or model as required by the API&#10;    payload = {}&#10;    if MISTRAL_AGENT_ID:&#10;        payload[&quot;agent_id&quot;] = MISTRAL_AGENT_ID&#10;        print(f&quot;[DEBUG] Using agent_id: {MISTRAL_AGENT_ID}&quot;)&#10;    else:&#10;        payload[&quot;model&quot;] = MISTRAL_MODEL&#10;        print(f&quot;[DEBUG] Using model: {MISTRAL_MODEL}&quot;)&#10;&#10;    async with httpx.AsyncClient() as client:&#10;        try:&#10;            response = await client.post(&#10;                &quot;https://api.mistral.ai/v1/conversations&quot;,&#10;                json=payload,&#10;                headers=headers&#10;            )&#10;            response.raise_for_status()&#10;            data = response.json()&#10;            conversation_id = data[&quot;id&quot;]&#10;            print(f&quot;[DEBUG] Created new conversation with ID: {conversation_id}&quot;)&#10;            return conversation_id&#10;        except Exception as e:&#10;            print(f&quot;[DEBUG] Error creating conversation: {str(e)}&quot;)&#10;            if hasattr(e, &quot;response&quot;) and e.response:&#10;                print(f&quot;[DEBUG] Response: {e.response.text}&quot;)&#10;            raise&#10;&#10;async def append_to_conversation(conversation_id, content, role=&quot;user&quot;):&#10;    &quot;&quot;&quot;Append a message to a conversation&quot;&quot;&quot;&#10;    print(f&quot;[DEBUG] Appending {role} message to conversation {conversation_id}&quot;)&#10;&#10;    headers = {&#10;        &quot;Authorization&quot;: f&quot;Bearer {MISTRAL_API_KEY}&quot;,&#10;        &quot;Content-Type&quot;: &quot;application/json&quot;&#10;    }&#10;&#10;    payload = {&#10;        &quot;messages&quot;: [&#10;            {&#10;                &quot;role&quot;: role,&#10;                &quot;content&quot;: content&#10;            }&#10;        ]&#10;    }&#10;&#10;    async with httpx.AsyncClient() as client:&#10;        try:&#10;            response = await client.post(&#10;                f&quot;https://api.mistral.ai/v1/conversations/{conversation_id}/messages&quot;,&#10;                json=payload,&#10;                headers=headers&#10;            )&#10;            response.raise_for_status()&#10;            return response.json()&#10;        except Exception as e:&#10;            print(f&quot;[DEBUG] Error appending message: {str(e)}&quot;)&#10;            if hasattr(e, &quot;response&quot;) and e.response:&#10;                print(f&quot;[DEBUG] Response: {e.response.text}&quot;)&#10;            raise&#10;&#10;async def generate_agent_response(conversation_id):&#10;    &quot;&quot;&quot;Generate a response from the agent for a conversation&quot;&quot;&quot;&#10;    print(f&quot;[DEBUG] Generating agent response for conversation {conversation_id}&quot;)&#10;&#10;    headers = {&#10;        &quot;Authorization&quot;: f&quot;Bearer {MISTRAL_API_KEY}&quot;,&#10;        &quot;Content-Type&quot;: &quot;application/json&quot;&#10;    }&#10;&#10;    # We'll use the standard conversation completion endpoint since we already&#10;    # specified the agent_id or model during conversation creation&#10;    url = f&quot;https://api.mistral.ai/v1/conversations/{conversation_id}/messages&quot;&#10;    print(f&quot;[DEBUG] Request URL: {url}&quot;)&#10;&#10;    async with httpx.AsyncClient(timeout=60.0) as client:&#10;        try:&#10;            # For simplicity, we'll use a non-streaming approach first&#10;            response = await client.post(&#10;                url,&#10;                headers=headers,&#10;                json={&quot;role&quot;: &quot;assistant&quot;}&#10;            )&#10;            response.raise_for_status()&#10;            data = response.json()&#10;&#10;            # Extract the assistant's response&#10;            if &quot;content&quot; in data:&#10;                content = data[&quot;content&quot;]&#10;                print(f&quot;[DEBUG] Got response: {content[:100]}...&quot;)&#10;                return content&#10;            else:&#10;                print(f&quot;[DEBUG] Unexpected response format: {data}&quot;)&#10;                raise ValueError(&quot;Unexpected response format from Mistral API&quot;)&#10;&#10;        except Exception as e:&#10;            print(f&quot;[DEBUG] Error generating response: {str(e)}&quot;)&#10;            if hasattr(e, &quot;response&quot;) and e.response:&#10;                print(f&quot;[DEBUG] Response: {e.response.text}&quot;)&#10;            raise&#10;&#10;async def get_conversation_messages(conversation_id):&#10;    &quot;&quot;&quot;Get all messages in a conversation&quot;&quot;&quot;&#10;    print(f&quot;[DEBUG] Getting messages for conversation {conversation_id}&quot;)&#10;&#10;    headers = {&#10;        &quot;Authorization&quot;: f&quot;Bearer {MISTRAL_API_KEY}&quot;&#10;    }&#10;&#10;    async with httpx.AsyncClient() as client:&#10;        response = await client.get(&#10;            f&quot;https://api.mistral.ai/v1/conversations/{conversation_id}/messages&quot;,&#10;            headers=headers&#10;        )&#10;        response.raise_for_status()&#10;        return response.json()[&quot;data&quot;]&#10;&#10;@bot.event&#10;async def on_ready():&#10;    print(f&quot;[READY] Logged in as {bot.user}&quot;)&#10;    channel = bot.get_channel(ALLOWED_CHANNEL_ID)&#10;    if channel:&#10;        await channel.send(&quot;Hello! I am online using Mistral's Conversations and Agents API.&quot;)&#10;&#10;@bot.event&#10;async def on_message(message):&#10;    if message.author.bot or message.channel.id != ALLOWED_CHANNEL_ID:&#10;        return&#10;    await bot.process_commands(message)&#10;&#10;@bot.command(name=&quot;nai&quot;)&#10;async def nai_command(ctx, *, prompt: str = None):&#10;    if ctx.channel.id != ALLOWED_CHANNEL_ID:&#10;        return&#10;&#10;    if not prompt:&#10;        await ctx.send(&quot;⚠️ Please provide a prompt after /nai&quot;)&#10;        return&#10;&#10;    print(f&quot;[DEBUG] Processing /nai command from {ctx.author}: {prompt[:50]}...&quot;)&#10;&#10;    user_id = str(ctx.author.id)&#10;    start_time = time.perf_counter()&#10;&#10;    try:&#10;        # Get or create a conversation for this user&#10;        if user_id not in user_conversations:&#10;            conversation_id = await create_conversation()&#10;            user_conversations[user_id] = conversation_id&#10;&#10;            # Add initial system message to set expectations&#10;            await append_to_conversation(&#10;                conversation_id,&#10;                &quot;&quot;&quot;You're a Discord AI assistant.&#10;- Answer concisely and helpfully&#10;- No prefixes like &quot;Assistant:&quot; in your response&#10;- Be polite and respectful&quot;&quot;&quot;,&#10;                role=&quot;system&quot;&#10;            )&#10;        else:&#10;            conversation_id = user_conversations[user_id]&#10;            print(f&quot;[DEBUG] Using existing conversation with ID: {conversation_id}&quot;)&#10;&#10;        # Format the prompt with user information&#10;        formatted_prompt = f&quot;{ctx.author}: {prompt}&quot;&#10;&#10;        # Add user message to the conversation&#10;        await append_to_conversation(conversation_id, formatted_prompt)&#10;        print(f&quot;[DEBUG] Added user message to conversation&quot;)&#10;&#10;        # Send typing indicator while generating&#10;        async with ctx.typing():&#10;            # Get response from Mistral&#10;            try:&#10;                reply = await generate_agent_response(conversation_id)&#10;                if not reply or not reply.strip():&#10;                    raise ValueError(&quot;Empty response from Mistral&quot;)&#10;&#10;                # Calculate elapsed time&#10;                elapsed_time = (time.perf_counter() - start_time) * 1000&#10;                print(f&quot;[DEBUG] Got response from Mistral (took {elapsed_time:.2f} ms)&quot;)&#10;&#10;                # Send the response&#10;                await ctx.send(reply)&#10;&#10;                # Store in memory for stats purposes&#10;                memory.store_message(prompt, str(ctx.author), role=&quot;user&quot;)&#10;                memory.store_message(reply, str(ctx.me), role=&quot;bot&quot;)&#10;                print(&quot;[DEBUG] Messages stored in memory (for stats only)&quot;)&#10;&#10;            except Exception as e:&#10;                print(f&quot;[DEBUG] Error during response generation: {str(e)}&quot;)&#10;                await ctx.send(&quot;⚠️ Error communicating with Mistral.&quot;)&#10;&#10;    except Exception as e:&#10;        print(f&quot;[DEBUG] Error in nai_command: {str(e)}&quot;)&#10;        await ctx.send(&quot;⚠️ An error occurred while processing the command.&quot;)&#10;&#10;    print(&quot;---&quot;)&#10;&#10;@bot.command(name=&quot;new&quot;)&#10;async def new_conversation(ctx):&#10;    &quot;&quot;&quot;Start a new conversation, forgetting previous context&quot;&quot;&quot;&#10;    user_id = str(ctx.author.id)&#10;&#10;    try:&#10;        # Create a new conversation&#10;        conversation_id = await create_conversation()&#10;        user_conversations[user_id] = conversation_id&#10;&#10;        # Add initial system message&#10;        await append_to_conversation(&#10;            conversation_id,&#10;            &quot;&quot;&quot;You're a Discord AI assistant.&#10;- Answer concisely and helpfully&#10;- No prefixes like &quot;Assistant:&quot; in your response&#10;- Be polite and respectful&quot;&quot;&quot;,&#10;            role=&quot;system&quot;&#10;        )&#10;&#10;        await ctx.send(&quot;Started a new conversation! Your previous conversation history has been cleared.&quot;)&#10;    except Exception as e:&#10;        print(f&quot;[DEBUG] Error creating new conversation: {str(e)}&quot;)&#10;        await ctx.send(&quot;⚠️ Error creating new conversation.&quot;)&#10;&#10;    print(&quot;---&quot;)&#10;&#10;@bot.command(name=&quot;history&quot;)&#10;async def history_command(ctx, limit: int = 5):&#10;    &quot;&quot;&quot;Show recent conversation history&quot;&quot;&quot;&#10;    user_id = str(ctx.author.id)&#10;&#10;    if user_id not in user_conversations:&#10;        await ctx.send(&quot;You don't have an active conversation. Start one with `/nai`.&quot;)&#10;        return&#10;&#10;    try:&#10;        conversation_id = user_conversations[user_id]&#10;        messages = await get_conversation_messages(conversation_id)&#10;&#10;        # Filter out system messages and limit the number of messages&#10;        filtered_messages = [msg for msg in messages if msg[&quot;role&quot;] != &quot;system&quot;][:limit]&#10;&#10;        if not filtered_messages:&#10;            await ctx.send(&quot;No messages in the current conversation yet.&quot;)&#10;            return&#10;&#10;        history_text = &quot;**Recent Conversation History:**\n\n&quot;&#10;        for msg in reversed(filtered_messages):  # Show oldest first&#10;            role = &quot;&quot; if msg[&quot;role&quot;] == &quot;assistant&quot; else &quot;&quot;&#10;            history_text += f&quot;{role} **{msg['role']}**: {msg['content'][:200]}&quot;&#10;            if len(msg['content']) &gt; 200:&#10;                history_text += &quot;...&quot;&#10;            history_text += &quot;\n\n&quot;&#10;&#10;        await ctx.send(history_text)&#10;    except Exception as e:&#10;        print(f&quot;[DEBUG] Error retrieving conversation history: {str(e)}&quot;)&#10;        await ctx.send(&quot;⚠️ Error retrieving conversation history.&quot;)&#10;&#10;    print(&quot;---&quot;)&#10;&#10;@bot.command(name=&quot;stats&quot;)&#10;async def stats_command(ctx):&#10;    &quot;&quot;&quot;Show conversation statistics&quot;&quot;&quot;&#10;    user_id = str(ctx.author.id)&#10;&#10;    stats_text = &quot;**Conversation Stats:**\n&quot;&#10;&#10;    # Get memory statistics&#10;    with sqlite3.connect(memory.db_path) as conn:&#10;        cursor = conn.execute(&quot;SELECT COUNT(*) FROM messages&quot;)&#10;        total_messages = cursor.fetchone()[0]&#10;&#10;        cursor = conn.execute(&quot;SELECT COUNT(*) FROM messages WHERE role='user'&quot;)&#10;        user_messages = cursor.fetchone()[0]&#10;&#10;        cursor = conn.execute(&quot;SELECT COUNT(*) FROM messages WHERE role='bot'&quot;)&#10;        bot_messages = cursor.fetchone()[0]&#10;&#10;    stats_text += f&quot; Total messages in memory: {total_messages}\n&quot;&#10;    stats_text += f&quot; User messages: {user_messages}\n&quot;&#10;    stats_text += f&quot; Bot messages: {bot_messages}\n&quot;&#10;&#10;    # Add conversation status&#10;    if user_id in user_conversations:&#10;        stats_text += f&quot; Active conversation ID: {user_conversations[user_id]}\n&quot;&#10;    else:&#10;        stats_text += &quot; No active conversation\n&quot;&#10;&#10;    stats_text += &quot;\n*Using Mistral's Conversations and Agents API*&quot;&#10;&#10;    await ctx.send(stats_text)&#10;&#10;bot.run(DISCORD_TOKEN)&#10;" />
              <option name="updatedContent" value="import os&#10;import json&#10;import discord&#10;from discord.ext import commands&#10;import httpx&#10;import time&#10;import asyncio&#10;from dotenv import load_dotenv&#10;from memory import ConversationMemory&#10;import sqlite3&#10;&#10;load_dotenv()&#10;&#10;DISCORD_TOKEN = os.getenv(&quot;DISCORD_TOKEN&quot;)&#10;MISTRAL_API_KEY = os.getenv(&quot;MISTRAL_API_KEY&quot;)&#10;ALLOWED_CHANNEL_ID = int(os.getenv(&quot;ALLOWED_CHANNEL_ID&quot;))&#10;&#10;# Define the agent ID to use or fallback to model name if not specified&#10;MISTRAL_AGENT_ID = os.getenv(&quot;MISTRAL_AGENT_ID&quot;)&#10;MISTRAL_MODEL = os.getenv(&quot;MISTRAL_MODEL&quot;, &quot;mistral-small&quot;)&#10;&#10;intents = discord.Intents.default()&#10;intents.message_content = True&#10;&#10;bot = commands.Bot(command_prefix=&quot;/&quot;, intents=intents)&#10;&#10;# Dictionary to store conversation IDs for each user&#10;user_conversations = {}&#10;&#10;# Initialize memory system for stats only&#10;memory = ConversationMemory()&#10;print(&quot;[DEBUG] Memory system initialized (used for stats only)&quot;)&#10;&#10;async def create_conversation():&#10;    &quot;&quot;&quot;Create a new conversation with Mistral Conversations API&quot;&quot;&quot;&#10;    print(&quot;[DEBUG] Creating new Mistral conversation&quot;)&#10;&#10;    headers = {&#10;        &quot;Authorization&quot;: f&quot;Bearer {MISTRAL_API_KEY}&quot;,&#10;        &quot;Content-Type&quot;: &quot;application/json&quot;&#10;    }&#10;    &#10;    # Based on the error message, we need to structure the payload differently&#10;    # The API expects either:&#10;    # 1. For agent: {&quot;agent_id&quot;: &quot;agent_id&quot;, &quot;inputs&quot;: {...}}&#10;    # 2. For model: {&quot;model&quot;: &quot;model_name&quot;, &quot;inputs&quot;: {...}}&#10;    &#10;    payload = {&quot;inputs&quot;: {}}  # Empty inputs for now&#10;    &#10;    if MISTRAL_AGENT_ID:&#10;        payload[&quot;agent_id&quot;] = MISTRAL_AGENT_ID&#10;        print(f&quot;[DEBUG] Using agent_id: {MISTRAL_AGENT_ID}&quot;)&#10;    else:&#10;        payload[&quot;model&quot;] = MISTRAL_MODEL&#10;        print(f&quot;[DEBUG] Using model: {MISTRAL_MODEL}&quot;)&#10;    &#10;    print(f&quot;[DEBUG] Conversation payload: {payload}&quot;)&#10;&#10;    # Let's try using the chat completions API instead for compatibility&#10;    try:&#10;        print(&quot;[DEBUG] Creating conversation using chat completions API&quot;)&#10;        async with httpx.AsyncClient() as client:&#10;            response = await client.post(&#10;                &quot;https://api.mistral.ai/v1/chat/completions&quot;,&#10;                json={&#10;                    &quot;model&quot;: MISTRAL_MODEL,&#10;                    &quot;messages&quot;: [&#10;                        {&#10;                            &quot;role&quot;: &quot;system&quot;,&#10;                            &quot;content&quot;: &quot;You're a Discord AI assistant. Answer concisely and helpfully. No prefixes like 'Assistant:' in your response. Be polite and respectful.&quot;&#10;                        }&#10;                    ]&#10;                },&#10;                headers=headers&#10;            )&#10;            response.raise_for_status()&#10;            &#10;            # Generate a unique ID for the conversation&#10;            conversation_id = f&quot;local_{time.time()}&quot;&#10;            print(f&quot;[DEBUG] Created local conversation with ID: {conversation_id}&quot;)&#10;            &#10;            # Store initial system message in memory&#10;            return conversation_id&#10;    except Exception as e:&#10;        print(f&quot;[DEBUG] Error creating conversation using chat API: {str(e)}&quot;)&#10;        if hasattr(e, &quot;response&quot;) and e.response:&#10;            print(f&quot;[DEBUG] Response: {e.response.text}&quot;)&#10;        raise&#10;&#10;async def append_to_conversation(conversation_id, content, role=&quot;user&quot;):&#10;    &quot;&quot;&quot;Store a message in the conversation history&quot;&quot;&quot;&#10;    print(f&quot;[DEBUG] Appending {role} message to conversation {conversation_id}&quot;)&#10;    &#10;    # Store message in the memory system&#10;    memory.store_message(content, &quot;User&quot; if role == &quot;user&quot; else &quot;Assistant&quot;, role=role)&#10;    return {&quot;success&quot;: True}&#10;&#10;async def generate_agent_response(conversation_id, user_message):&#10;    &quot;&quot;&quot;Generate a response using the Mistral chat completions API&quot;&quot;&quot;&#10;    print(f&quot;[DEBUG] Generating response for conversation {conversation_id}&quot;)&#10;&#10;    headers = {&#10;        &quot;Authorization&quot;: f&quot;Bearer {MISTRAL_API_KEY}&quot;,&#10;        &quot;Content-Type&quot;: &quot;application/json&quot;&#10;    }&#10;&#10;    # Get recent messages from memory to build context&#10;    recent_messages = memory.get_recent_messages(15)&#10;    &#10;    # Format messages for the Mistral API&#10;    messages = [&#10;        {&#10;            &quot;role&quot;: &quot;system&quot;,&#10;            &quot;content&quot;: &quot;You're a Discord AI assistant. Answer concisely and helpfully. No prefixes like 'Assistant:' in your response. Be polite and respectful.&quot;&#10;        }&#10;    ]&#10;    &#10;    # Add recent messages to provide context&#10;    for msg in recent_messages:&#10;        role = &quot;assistant&quot; if msg[&quot;role&quot;] == &quot;bot&quot; else &quot;user&quot;&#10;        messages.append({&#10;            &quot;role&quot;: role,&#10;            &quot;content&quot;: msg[&quot;content&quot;]&#10;        })&#10;    &#10;    # Add the current user message&#10;    messages.append({&#10;        &quot;role&quot;: &quot;user&quot;,&#10;        &quot;content&quot;: user_message&#10;    })&#10;    &#10;    payload = {&#10;        &quot;model&quot;: MISTRAL_MODEL,&#10;        &quot;messages&quot;: messages,&#10;        &quot;temperature&quot;: 0.7&#10;    }&#10;    &#10;    print(f&quot;[DEBUG] Sending {len(messages)} messages to Mistral API&quot;)&#10;&#10;    async with httpx.AsyncClient(timeout=60.0) as client:&#10;        try:&#10;            response = await client.post(&#10;                &quot;https://api.mistral.ai/v1/chat/completions&quot;,&#10;                json=payload,&#10;                headers=headers&#10;            )&#10;            response.raise_for_status()&#10;            data = response.json()&#10;            content = data[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]&#10;            print(f&quot;[DEBUG] Got response from Mistral: {content[:100]}...&quot;)&#10;            return content&#10;        except Exception as e:&#10;            print(f&quot;[DEBUG] Error generating response: {str(e)}&quot;)&#10;            if hasattr(e, &quot;response&quot;) and e.response:&#10;                print(f&quot;[DEBUG] Response: {e.response.text}&quot;)&#10;            raise&#10;&#10;async def get_conversation_messages(conversation_id, limit=10):&#10;    &quot;&quot;&quot;Get recent messages from memory&quot;&quot;&quot;&#10;    print(f&quot;[DEBUG] Getting messages for conversation {conversation_id}&quot;)&#10;    &#10;    messages = memory.get_recent_messages(limit)&#10;    return [&#10;        {&#10;            &quot;role&quot;: &quot;assistant&quot; if msg[&quot;role&quot;] == &quot;bot&quot; else &quot;user&quot;,&#10;            &quot;content&quot;: msg[&quot;content&quot;]&#10;        }&#10;        for msg in messages&#10;    ]&#10;&#10;@bot.event&#10;async def on_ready():&#10;    print(f&quot;[READY] Logged in as {bot.user}&quot;)&#10;    channel = bot.get_channel(ALLOWED_CHANNEL_ID)&#10;    if channel:&#10;        await channel.send(&quot;Hello! I am online using Mistral's chat completions API.&quot;)&#10;&#10;@bot.event&#10;async def on_message(message):&#10;    if message.author.bot or message.channel.id != ALLOWED_CHANNEL_ID:&#10;        return&#10;    await bot.process_commands(message)&#10;&#10;@bot.command(name=&quot;nai&quot;)&#10;async def nai_command(ctx, *, prompt: str = None):&#10;    if ctx.channel.id != ALLOWED_CHANNEL_ID:&#10;        return&#10;&#10;    if not prompt:&#10;        await ctx.send(&quot;⚠️ Please provide a prompt after /nai&quot;)&#10;        return&#10;&#10;    print(f&quot;[DEBUG] Processing /nai command from {ctx.author}: {prompt[:50]}...&quot;)&#10;&#10;    user_id = str(ctx.author.id)&#10;    start_time = time.perf_counter()&#10;&#10;    try:&#10;        # Get or create a conversation for this user&#10;        if user_id not in user_conversations:&#10;            conversation_id = await create_conversation()&#10;            user_conversations[user_id] = conversation_id&#10;            print(f&quot;[DEBUG] Created new conversation for user {user_id}&quot;)&#10;        else:&#10;            conversation_id = user_conversations[user_id]&#10;            print(f&quot;[DEBUG] Using existing conversation with ID: {conversation_id}&quot;)&#10;&#10;        # Format the prompt with user information&#10;        formatted_prompt = f&quot;{ctx.author}: {prompt}&quot;&#10;&#10;        # Add user message to the conversation&#10;        await append_to_conversation(conversation_id, formatted_prompt, role=&quot;user&quot;)&#10;        print(f&quot;[DEBUG] Added user message to conversation&quot;)&#10;&#10;        # Send typing indicator while generating&#10;        async with ctx.typing():&#10;            # Get response from Mistral&#10;            try:&#10;                reply = await generate_agent_response(conversation_id, formatted_prompt)&#10;                if not reply or not reply.strip():&#10;                    raise ValueError(&quot;Empty response from Mistral&quot;)&#10;&#10;                # Calculate elapsed time&#10;                elapsed_time = (time.perf_counter() - start_time) * 1000&#10;                print(f&quot;[DEBUG] Got response from Mistral (took {elapsed_time:.2f} ms)&quot;)&#10;&#10;                # Send the response&#10;                await ctx.send(reply)&#10;&#10;                # Store bot response in memory&#10;                await append_to_conversation(conversation_id, reply, role=&quot;bot&quot;)&#10;                print(&quot;[DEBUG] Stored bot response in memory&quot;)&#10;&#10;            except Exception as e:&#10;                print(f&quot;[DEBUG] Error during response generation: {str(e)}&quot;)&#10;                await ctx.send(&quot;⚠️ Error communicating with Mistral.&quot;)&#10;&#10;    except Exception as e:&#10;        print(f&quot;[DEBUG] Error in nai_command: {str(e)}&quot;)&#10;        await ctx.send(&quot;⚠️ An error occurred while processing the command.&quot;)&#10;&#10;    print(&quot;---&quot;)&#10;&#10;@bot.command(name=&quot;new&quot;)&#10;async def new_conversation(ctx):&#10;    &quot;&quot;&quot;Start a new conversation, forgetting previous context&quot;&quot;&quot;&#10;    user_id = str(ctx.author.id)&#10;&#10;    try:&#10;        # Create a new conversation&#10;        conversation_id = await create_conversation()&#10;        user_conversations[user_id] = conversation_id&#10;&#10;        # Add initial system message for stats display&#10;        await append_to_conversation(&#10;            conversation_id,&#10;            &quot;&quot;&quot;You're a Discord AI assistant.&#10;- Answer concisely and helpfully&#10;- No prefixes like &quot;Assistant:&quot; in your response&#10;- Be polite and respectful&quot;&quot;&quot;,&#10;            role=&quot;system&quot;&#10;        )&#10;&#10;        await ctx.send(&quot;Started a new conversation! Your previous conversation history has been cleared.&quot;)&#10;    except Exception as e:&#10;        print(f&quot;[DEBUG] Error creating new conversation: {str(e)}&quot;)&#10;        await ctx.send(&quot;⚠️ Error creating new conversation.&quot;)&#10;&#10;    print(&quot;---&quot;)&#10;&#10;@bot.command(name=&quot;history&quot;)&#10;async def history_command(ctx, limit: int = 5):&#10;    &quot;&quot;&quot;Show recent conversation history&quot;&quot;&quot;&#10;    user_id = str(ctx.author.id)&#10;&#10;    if user_id not in user_conversations:&#10;        await ctx.send(&quot;You don't have an active conversation. Start one with `/nai`.&quot;)&#10;        return&#10;&#10;    try:&#10;        conversation_id = user_conversations[user_id]&#10;        messages = await get_conversation_messages(conversation_id, limit)&#10;&#10;        if not messages:&#10;            await ctx.send(&quot;No messages in the current conversation yet.&quot;)&#10;            return&#10;&#10;        history_text = &quot;**Recent Conversation History:**\n\n&quot;&#10;        for msg in messages:  # Messages should already be in correct order&#10;            role = &quot;&quot; if msg[&quot;role&quot;] == &quot;assistant&quot; else &quot;&quot;&#10;            history_text += f&quot;{role} **{msg['role']}**: {msg['content'][:200]}&quot;&#10;            if len(msg['content']) &gt; 200:&#10;                history_text += &quot;...&quot;&#10;            history_text += &quot;\n\n&quot;&#10;&#10;        await ctx.send(history_text)&#10;    except Exception as e:&#10;        print(f&quot;[DEBUG] Error retrieving conversation history: {str(e)}&quot;)&#10;        await ctx.send(&quot;⚠️ Error retrieving conversation history.&quot;)&#10;&#10;    print(&quot;---&quot;)&#10;&#10;@bot.command(name=&quot;stats&quot;)&#10;async def stats_command(ctx):&#10;    &quot;&quot;&quot;Show conversation statistics&quot;&quot;&quot;&#10;    user_id = str(ctx.author.id)&#10;&#10;    stats_text = &quot;**Conversation Stats:**\n&quot;&#10;&#10;    # Get memory statistics&#10;    with sqlite3.connect(memory.db_path) as conn:&#10;        cursor = conn.execute(&quot;SELECT COUNT(*) FROM messages&quot;)&#10;        total_messages = cursor.fetchone()[0]&#10;&#10;        cursor = conn.execute(&quot;SELECT COUNT(*) FROM messages WHERE role='user'&quot;)&#10;        user_messages = cursor.fetchone()[0]&#10;&#10;        cursor = conn.execute(&quot;SELECT COUNT(*) FROM messages WHERE role='bot'&quot;)&#10;        bot_messages = cursor.fetchone()[0]&#10;&#10;    stats_text += f&quot; Total messages in memory: {total_messages}\n&quot;&#10;    stats_text += f&quot; User messages: {user_messages}\n&quot;&#10;    stats_text += f&quot; Bot messages: {bot_messages}\n&quot;&#10;&#10;    # Add conversation status&#10;    if user_id in user_conversations:&#10;        stats_text += f&quot; Active conversation ID: {user_conversations[user_id]}\n&quot;&#10;    else:&#10;        stats_text += &quot; No active conversation\n&quot;&#10;&#10;    stats_text += &quot;\n*Using Mistral's chat completions API with memory*&quot;&#10;&#10;    await ctx.send(stats_text)&#10;&#10;bot.run(DISCORD_TOKEN)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/bot/bot.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/bot/bot.py" />
              <option name="originalContent" value="import os&#10;import discord&#10;from langchain.chat_models import init_chat_model&#10;from langchain_core.messages import HumanMessage&#10;from config import DISCORD_TOKEN, ALLOWED_CHANNEL_ID, MISTRAL_API_KEY&#10;from memory import MessageMemory&#10;from prompt import prompt_template&#10;from search import duckduckgo_search&#10;&#10;# Setup Discord&#10;intents = discord.Intents.default()&#10;intents.message_content = True&#10;client = discord.Client(intents=intents)&#10;&#10;# Define the chat model&#10;model = init_chat_model(&quot;mistral-tiny&quot;, model_provider=&quot;mistralai&quot;, mistral_api_key=MISTRAL_API_KEY, temperature=0.7)&#10;memory = MessageMemory()&#10;&#10;# Define a prompt template to structure the context&#10;prompt_template = PromptTemplate(&#10;    input_variables=[&quot;long_term&quot;, &quot;short_term&quot;, &quot;query&quot;],&#10;    template=(&#10;        &quot;You are a Discord chat bot. You are given some context before the user's querry. Only take it into consideration if it makes sense.  \n&quot;&#10;        &quot;Relevant long-term context:\n{long_term}\n\n&quot;&#10;        &quot;Recent short-term conversation:\n{short_term}\n\n&quot;&#10;        &quot;Current user query:\n{query}\n&quot;&#10;        &quot;Respond to the user using all available context.&quot;&#10;    )&#10;)&#10;&#10;def duckduckgo_search(query, max_results=5):&#10;    results = []&#10;    with DDGS() as ddgs:&#10;        for r in ddgs.text(query, max_results=max_results):&#10;            results.append(r.get(&quot;body&quot;, &quot;&quot;) or r.get(&quot;title&quot;, &quot;&quot;))&#10;    return &quot;\n&quot;.join(results) if results else &quot;No relevant results found.&quot;&#10;&#10;@client.event&#10;async def on_ready():&#10;    print(f&quot;[READY] Logged in as {client.user}&quot;)&#10;&#10;@client.event&#10;async def on_message(message):&#10;    if message.author.bot or message.channel.id != ALLOWED_CHANNEL_ID:&#10;        return&#10;&#10;    try:&#10;        async with message.channel.typing():&#10;            user_id = str(message.author.id)&#10;            short_term = memory.get_short_term_history()&#10;            long_term = memory.get_relevant_long_term_history(&#10;                message.content,&#10;                user_id=user_id,&#10;                distance_threshold=0.7,&#10;                user_weight=0.5  # boost for same user&#10;            )&#10;&#10;            short_term_str = &quot;\n&quot;.join(&#10;                [f&quot;{'User' if isinstance(m, HumanMessage) else 'Bot'}: {m.content}&quot; for m in short_term]&#10;            )&#10;            long_term_str = &quot;\n&quot;.join(&#10;                [f&quot;{'User' if isinstance(m, HumanMessage) else 'Bot'}: {m.content}&quot; for m in long_term]&#10;            )&#10;&#10;            # DuckDuckGo search trigger&#10;            internet_context = &quot;&quot;&#10;            if &quot;search:&quot; in message.content.lower() or &quot;look up&quot; in message.content.lower():&#10;                search_query = message.content.split(&quot;search:&quot;, 1)[-1].strip() if &quot;search:&quot; in message.content.lower() else message.content.split(&quot;look up&quot;, 1)[-1].strip()&#10;                print(f&quot;[DEBUG] Performing DuckDuckGo search for: {search_query}&quot;)&#10;                search_results = duckduckgo_search(search_query)&#10;                internet_context = f&quot;\nInternet Search Results (DuckDuckGo):\n{search_results}\n&quot;&#10;&#10;            prompt = prompt_template.format(&#10;                long_term=long_term_str + internet_context,&#10;                short_term=short_term_str,&#10;                query=message.content&#10;            )&#10;&#10;            print(&quot;[DEBUG] Prompt sent to AI:\n&quot;, prompt)&#10;&#10;            response = model.invoke([HumanMessage(content=prompt)])&#10;            # Now save the current user message and bot response&#10;            memory.save_message(str(message.author.id), message.content, is_bot=False)&#10;            memory.save_message(str(client.user.id), response.content, is_bot=True)&#10;            await message.channel.send(response.content)&#10;&#10;    except Exception as e:&#10;        print(&quot;❌ Error:&quot;, e)&#10;        await message.channel.send(&quot;⚠️ An error occurred while processing your message.&quot;)&#10;&#10;client.run(DISCORD_TOKEN)&#10;" />
              <option name="updatedContent" value="import os&#10;import discord&#10;from langchain.chat_models import init_chat_model&#10;from langchain_core.messages import HumanMessage&#10;from config import DISCORD_TOKEN, ALLOWED_CHANNEL_ID, MISTRAL_API_KEY&#10;from memory import MessageMemory&#10;from prompt import prompt_template&#10;from search import duckduckgo_search&#10;&#10;# Setup Discord&#10;intents = discord.Intents.default()&#10;intents.message_content = True&#10;client = discord.Client(intents=intents)&#10;&#10;# Define the chat model&#10;model = init_chat_model(&quot;mistral-tiny&quot;, model_provider=&quot;mistralai&quot;, mistral_api_key=MISTRAL_API_KEY, temperature=0.7)&#10;memory = MessageMemory()&#10;&#10;@client.event&#10;async def on_ready():&#10;    print(f&quot;[READY] Logged in as {client.user}&quot;)&#10;&#10;@client.event&#10;async def on_message(message):&#10;    if message.author.bot or message.channel.id != ALLOWED_CHANNEL_ID:&#10;        return&#10;&#10;    try:&#10;        async with message.channel.typing():&#10;            user_id = str(message.author.id)&#10;            short_term = memory.get_short_term_history()&#10;            long_term = memory.get_relevant_long_term_history(&#10;                message.content,&#10;                user_id=user_id,&#10;                distance_threshold=0.7,&#10;                user_weight=0.5  # boost for same user&#10;            )&#10;&#10;            short_term_str = &quot;\n&quot;.join(&#10;                [f&quot;{'User' if isinstance(m, HumanMessage) else 'Bot'}: {m.content}&quot; for m in short_term]&#10;            )&#10;            long_term_str = &quot;\n&quot;.join(&#10;                [f&quot;{'User' if isinstance(m, HumanMessage) else 'Bot'}: {m.content}&quot; for m in long_term]&#10;            )&#10;&#10;            # DuckDuckGo search trigger&#10;            internet_context = &quot;&quot;&#10;            if &quot;search:&quot; in message.content.lower() or &quot;look up&quot; in message.content.lower():&#10;                search_query = message.content.split(&quot;search:&quot;, 1)[-1].strip() if &quot;search:&quot; in message.content.lower() else message.content.split(&quot;look up&quot;, 1)[-1].strip()&#10;                print(f&quot;[DEBUG] Performing DuckDuckGo search for: {search_query}&quot;)&#10;                search_results = duckduckgo_search(search_query)&#10;                internet_context = f&quot;\nInternet Search Results (DuckDuckGo):\n{search_results}\n&quot;&#10;&#10;            prompt = prompt_template.format(&#10;                long_term=long_term_str + internet_context,&#10;                short_term=short_term_str,&#10;                query=message.content&#10;            )&#10;&#10;            print(&quot;[DEBUG] Prompt sent to AI:\n&quot;, prompt)&#10;&#10;            response = model.invoke([HumanMessage(content=prompt)])&#10;            # Now save the current user message and bot response&#10;            memory.save_message(str(message.author.id), message.content, is_bot=False)&#10;            memory.save_message(str(client.user.id), response.content, is_bot=True)&#10;            await message.channel.send(response.content)&#10;&#10;    except Exception as e:&#10;        print(&quot;❌ Error:&quot;, e)&#10;        await message.channel.send(&quot;⚠️ An error occurred while processing your message.&quot;)&#10;&#10;client.run(DISCORD_TOKEN)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/bot/memory.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/bot/memory.py" />
              <option name="originalContent" value="import sqlite3&#10;import os&#10;import numpy as np&#10;import faiss&#10;from langchain_huggingface import HuggingFaceEmbeddings&#10;from datetime import datetime&#10;from langchain.memory import ConversationBufferWindowMemory&#10;from langchain_core.messages import HumanMessage, AIMessage&#10;from sklearn.metrics.pairwise import cosine_similarity&#10;from sklearn.cluster import KMeans&#10;import time&#10;&#10;class MessageMemory:&#10;    def __init__(self, db_path=&quot;data/message_history.db&quot;, index_path=&quot;data/faiss_index&quot;):&#10;        self.db_path = db_path&#10;        self.index_path = index_path&#10;        # Explicitly set model_name to avoid deprecation warning&#10;        self.embeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;)&#10;        self.dimension = 384  # Default dimension for this model&#10;&#10;        self._init_db()&#10;        self._init_faiss()&#10;&#10;        # Short-term memory: last k messages&#10;        self.short_term_memory = ConversationBufferWindowMemory(&#10;            k=7,&#10;            return_messages=True,&#10;            memory_key=&quot;chat_history&quot;&#10;        )&#10;&#10;    def _init_db(self):&#10;        with sqlite3.connect(self.db_path) as conn:&#10;            conn.execute('''&#10;            CREATE TABLE IF NOT EXISTS messages (&#10;                id INTEGER PRIMARY KEY AUTOINCREMENT,&#10;                user_id TEXT,&#10;                message TEXT,&#10;                is_bot BOOLEAN,&#10;                timestamp TEXT,&#10;                embedding_id INTEGER&#10;            )&#10;            ''')&#10;            conn.commit()&#10;&#10;    def _init_faiss(self):&#10;        if os.path.exists(f&quot;{self.index_path}.index&quot;):&#10;            self.index = faiss.read_index(f&quot;{self.index_path}.index&quot;)&#10;        else:&#10;            self.index = faiss.IndexFlatL2(self.dimension)&#10;&#10;    def save_message(self, user_id, message, is_bot=False):&#10;        print(f&quot;[DEBUG] Saving message to DB | user_id: {user_id} | is_bot: {is_bot} | message: {message[:80]}&quot;)&#10;        embedding = self.embeddings.embed_query(message)&#10;        embedding_id = self.index.ntotal&#10;        self.index.add(np.array([embedding], dtype='float32'))&#10;        with sqlite3.connect(self.db_path) as conn:&#10;            conn.execute(&#10;                &quot;INSERT INTO messages (user_id, message, is_bot, timestamp, embedding_id) VALUES (?, ?, ?, ?, ?)&quot;,&#10;                (user_id, message, is_bot, datetime.now().isoformat(), embedding_id)&#10;            )&#10;            conn.commit()&#10;        print(f&quot;[DEBUG] Message saved with embedding_id: {embedding_id}&quot;)&#10;        faiss.write_index(self.index, f&quot;{self.index_path}.index&quot;)&#10;&#10;        # Add to short-term memory&#10;        if is_bot:&#10;            self.short_term_memory.chat_memory.add_ai_message(message)&#10;        else:&#10;            self.short_term_memory.chat_memory.add_user_message(message)&#10;&#10;    def get_short_term_history(self):&#10;        # Returns last k messages as LangChain messages&#10;        return self.short_term_memory.load_memory_variables({})[&quot;chat_history&quot;]&#10;&#10;    def get_relevant_long_term_history(self, query, user_id=None, k=6, distance_threshold=0.7, user_weight=0.5, recency_weight=0.2, similarity_cutoff=0.92):&#10;        &quot;&quot;&quot;&#10;        Semantic search for relevant historical messages, weighted by semantic similarity and user match.&#10;        Filters out messages that are too similar to each other.&#10;        user_weight: additional weight (lower score) for messages from the same user.&#10;        similarity_cutoff: if two messages have cosine similarity above this, only one is kept.&#10;        &quot;&quot;&quot;&#10;        query_vector = self.embeddings.embed_query(query)&#10;        D, I = self.index.search(np.array([query_vector], dtype='float32'), k*2)  # get more candidates for filtering&#10;        candidates = []&#10;        candidate_embeddings = []&#10;        now = time.time()&#10;        with sqlite3.connect(self.db_path) as conn:&#10;            for idx, dist in zip(I[0], D[0]):&#10;                if idx == -1:&#10;                    continue&#10;                result = conn.execute(&#10;                    &quot;SELECT message, is_bot, user_id, timestamp FROM messages WHERE embedding_id = ?&quot;,&#10;                    (int(idx),)&#10;                ).fetchone()&#10;                if result:&#10;                    msg_user_id = result[2]&#10;                    msg_time = datetime.fromisoformat(result[3]).timestamp()&#10;                    recency_score = recency_weight * (1 - min((now - msg_time) / (60*60*24), 1))  # boost for messages within 24h&#10;                    effective_dist = dist - user_weight if user_id and msg_user_id == user_id else dist&#10;                    effective_dist -= recency_score&#10;                    print(f&quot;[DEBUG] idx: {idx}, raw_dist: {dist}, user_id: {msg_user_id}, effective_dist: {effective_dist}, recency_score: {recency_score}&quot;)&#10;                    if effective_dist &lt; distance_threshold:&#10;                        candidates.append((&#10;                            AIMessage(content=result[0]) if result[1] else HumanMessage(content=result[0]),&#10;                            self.embeddings.embed_query(result[0])&#10;                        ))&#10;                        candidate_embeddings.append(self.embeddings.embed_query(result[0]))&#10;&#10;        # Filter out messages that are too similar to each other&#10;        # Diversify context using clustering if enough candidates&#10;        if len(candidate_embeddings) &gt; k:&#10;            try:&#10;                n_clusters = min(k, len(candidate_embeddings))&#10;                kmeans = KMeans(n_clusters=n_clusters, n_init=&quot;auto&quot;, random_state=42)&#10;                labels = kmeans.fit_predict(candidate_embeddings)&#10;                selected = []&#10;                selected_embeddings = []&#10;                for cluster_id in range(n_clusters):&#10;                    idxs = [i for i, lbl in enumerate(labels) if lbl == cluster_id]&#10;                    # Pick the candidate closest to the cluster center&#10;                    center = kmeans.cluster_centers_[cluster_id]&#10;                    best_idx = min(idxs, key=lambda i: np.linalg.norm(candidate_embeddings[i] - center))&#10;                    msg, emb = candidates[best_idx]&#10;                    selected.append(msg)&#10;                    selected_embeddings.append(emb)&#10;                # If less than k clusters, fill up with remaining diverse candidates&#10;                if len(selected) &lt; k:&#10;                    for i, (msg, emb) in enumerate(candidates):&#10;                        if msg not in selected and len(selected) &lt; k:&#10;                            sims = cosine_similarity([emb], selected_embeddings)[0]&#10;                            if all(sim &lt; similarity_cutoff for sim in sims):&#10;                                selected.append(msg)&#10;                                selected_embeddings.append(emb)&#10;            except Exception as e:&#10;                print(f&quot;[DEBUG] Clustering failed: {e}&quot;)&#10;                # fallback to previous selection&#10;                selected = []&#10;                selected_embeddings = []&#10;                for msg, emb in candidates:&#10;                    if len(selected) == 0:&#10;                        selected.append(msg)&#10;                        selected_embeddings.append(emb)&#10;                    else:&#10;                        sims = cosine_similarity([emb], selected_embeddings)[0]&#10;                        if all(sim &lt; similarity_cutoff for sim in sims):&#10;                            selected.append(msg)&#10;                            selected_embeddings.append(emb)&#10;                    if len(selected) &gt;= k:&#10;                        break&#10;        else:&#10;            selected = []&#10;            selected_embeddings = []&#10;            for msg, emb in candidates:&#10;                if len(selected) == 0:&#10;                    selected.append(msg)&#10;                    selected_embeddings.append(emb)&#10;                else:&#10;                    sims = cosine_similarity([emb], selected_embeddings)[0]&#10;                    if all(sim &lt; similarity_cutoff for sim in sims):&#10;                        selected.append(msg)&#10;                        selected_embeddings.append(emb)&#10;                if len(selected) &gt;= k:&#10;                    break&#10;&#10;        return selected&#10;" />
              <option name="updatedContent" value="import warnings&#10;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning, module=&quot;torch.nn.modules.module&quot;)&#10;&#10;import sqlite3&#10;import os&#10;import numpy as np&#10;import faiss&#10;from langchain_huggingface import HuggingFaceEmbeddings&#10;from datetime import datetime&#10;from langchain.memory import ConversationBufferWindowMemory&#10;from langchain_core.messages import HumanMessage, AIMessage&#10;from sklearn.metrics.pairwise import cosine_similarity&#10;from sklearn.cluster import KMeans&#10;import time&#10;&#10;class MessageMemory:&#10;    def __init__(self, db_path=&quot;data/message_history.db&quot;, index_path=&quot;data/faiss_index&quot;):&#10;        self.db_path = db_path&#10;        self.index_path = index_path&#10;        # Explicitly set model_name to avoid deprecation warning&#10;        self.embeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;)&#10;        self.dimension = 384  # Default dimension for this model&#10;&#10;        self._init_db()&#10;        self._init_faiss()&#10;&#10;        # Short-term memory: last k messages&#10;        self.short_term_memory = ConversationBufferWindowMemory(&#10;            k=7,&#10;            return_messages=True,&#10;            memory_key=&quot;chat_history&quot;&#10;        )&#10;&#10;    def _init_db(self):&#10;        with sqlite3.connect(self.db_path) as conn:&#10;            conn.execute('''&#10;            CREATE TABLE IF NOT EXISTS messages (&#10;                id INTEGER PRIMARY KEY AUTOINCREMENT,&#10;                user_id TEXT,&#10;                message TEXT,&#10;                is_bot BOOLEAN,&#10;                timestamp TEXT,&#10;                embedding_id INTEGER&#10;            )&#10;            ''')&#10;            conn.commit()&#10;&#10;    def _init_faiss(self):&#10;        if os.path.exists(f&quot;{self.index_path}.index&quot;):&#10;            self.index = faiss.read_index(f&quot;{self.index_path}.index&quot;)&#10;        else:&#10;            self.index = faiss.IndexFlatL2(self.dimension)&#10;&#10;    def save_message(self, user_id, message, is_bot=False):&#10;        print(f&quot;[DEBUG] Saving message to DB | user_id: {user_id} | is_bot: {is_bot} | message: {message[:80]}&quot;)&#10;        embedding = self.embeddings.embed_query(message)&#10;        embedding_id = self.index.ntotal&#10;        self.index.add(np.array([embedding], dtype='float32'))&#10;        with sqlite3.connect(self.db_path) as conn:&#10;            conn.execute(&#10;                &quot;INSERT INTO messages (user_id, message, is_bot, timestamp, embedding_id) VALUES (?, ?, ?, ?, ?)&quot;,&#10;                (user_id, message, is_bot, datetime.now().isoformat(), embedding_id)&#10;            )&#10;            conn.commit()&#10;        print(f&quot;[DEBUG] Message saved with embedding_id: {embedding_id}&quot;)&#10;        faiss.write_index(self.index, f&quot;{self.index_path}.index&quot;)&#10;&#10;        # Add to short-term memory&#10;        if is_bot:&#10;            self.short_term_memory.chat_memory.add_ai_message(message)&#10;        else:&#10;            self.short_term_memory.chat_memory.add_user_message(message)&#10;&#10;    def get_short_term_history(self):&#10;        # Returns last k messages as LangChain messages&#10;        return self.short_term_memory.load_memory_variables({})[&quot;chat_history&quot;]&#10;&#10;    def get_relevant_long_term_history(self, query, user_id=None, k=6, distance_threshold=0.7, user_weight=0.5, recency_weight=0.2, similarity_cutoff=0.92):&#10;        &quot;&quot;&quot;&#10;        Semantic search for relevant historical messages, weighted by semantic similarity and user match.&#10;        Filters out messages that are too similar to each other.&#10;        user_weight: additional weight (lower score) for messages from the same user.&#10;        similarity_cutoff: if two messages have cosine similarity above this, only one is kept.&#10;        &quot;&quot;&quot;&#10;        query_vector = self.embeddings.embed_query(query)&#10;        D, I = self.index.search(np.array([query_vector], dtype='float32'), k*2)  # get more candidates for filtering&#10;        candidates = []&#10;        candidate_embeddings = []&#10;        now = time.time()&#10;        with sqlite3.connect(self.db_path) as conn:&#10;            for idx, dist in zip(I[0], D[0]):&#10;                if idx == -1:&#10;                    continue&#10;                result = conn.execute(&#10;                    &quot;SELECT message, is_bot, user_id, timestamp FROM messages WHERE embedding_id = ?&quot;,&#10;                    (int(idx),)&#10;                ).fetchone()&#10;                if result:&#10;                    msg_user_id = result[2]&#10;                    msg_time = datetime.fromisoformat(result[3]).timestamp()&#10;                    recency_score = recency_weight * (1 - min((now - msg_time) / (60*60*24), 1))  # boost for messages within 24h&#10;                    effective_dist = dist - user_weight if user_id and msg_user_id == user_id else dist&#10;                    effective_dist -= recency_score&#10;                    print(f&quot;[DEBUG] idx: {idx}, raw_dist: {dist}, user_id: {msg_user_id}, effective_dist: {effective_dist}, recency_score: {recency_score}&quot;)&#10;                    if effective_dist &lt; distance_threshold:&#10;                        candidates.append((&#10;                            AIMessage(content=result[0]) if result[1] else HumanMessage(content=result[0]),&#10;                            self.embeddings.embed_query(result[0])&#10;                        ))&#10;                        candidate_embeddings.append(self.embeddings.embed_query(result[0]))&#10;&#10;        # Filter out messages that are too similar to each other&#10;        # Diversify context using clustering if enough candidates&#10;        if len(candidate_embeddings) &gt; k:&#10;            try:&#10;                n_clusters = min(k, len(candidate_embeddings))&#10;                kmeans = KMeans(n_clusters=n_clusters, n_init=&quot;auto&quot;, random_state=42)&#10;                labels = kmeans.fit_predict(candidate_embeddings)&#10;                selected = []&#10;                selected_embeddings = []&#10;                for cluster_id in range(n_clusters):&#10;                    idxs = [i for i, lbl in enumerate(labels) if lbl == cluster_id]&#10;                    # Pick the candidate closest to the cluster center&#10;                    center = kmeans.cluster_centers_[cluster_id]&#10;                    best_idx = min(idxs, key=lambda i: np.linalg.norm(candidate_embeddings[i] - center))&#10;                    msg, emb = candidates[best_idx]&#10;                    selected.append(msg)&#10;                    selected_embeddings.append(emb)&#10;                # If less than k clusters, fill up with remaining diverse candidates&#10;                if len(selected) &lt; k:&#10;                    for i, (msg, emb) in enumerate(candidates):&#10;                        if msg not in selected and len(selected) &lt; k:&#10;                            sims = cosine_similarity([emb], selected_embeddings)[0]&#10;                            if all(sim &lt; similarity_cutoff for sim in sims):&#10;                                selected.append(msg)&#10;                                selected_embeddings.append(emb)&#10;            except Exception as e:&#10;                print(f&quot;[DEBUG] Clustering failed: {e}&quot;)&#10;                # fallback to previous selection&#10;                selected = []&#10;                selected_embeddings = []&#10;                for msg, emb in candidates:&#10;                    if len(selected) == 0:&#10;                        selected.append(msg)&#10;                        selected_embeddings.append(emb)&#10;                    else:&#10;                        sims = cosine_similarity([emb], selected_embeddings)[0]&#10;                        if all(sim &lt; similarity_cutoff for sim in sims):&#10;                            selected.append(msg)&#10;                            selected_embeddings.append(emb)&#10;                    if len(selected) &gt;= k:&#10;                        break&#10;        else:&#10;            selected = []&#10;            selected_embeddings = []&#10;            for msg, emb in candidates:&#10;                if len(selected) == 0:&#10;                    selected.append(msg)&#10;                    selected_embeddings.append(emb)&#10;                else:&#10;                    sims = cosine_similarity([emb], selected_embeddings)[0]&#10;                    if all(sim &lt; similarity_cutoff for sim in sims):&#10;                        selected.append(msg)&#10;                        selected_embeddings.append(emb)&#10;                if len(selected) &gt;= k:&#10;                    break&#10;&#10;        return selected" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/bot/prompt.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/bot/prompt.py" />
              <option name="updatedContent" value="from langchain.prompts import PromptTemplate&#10;&#10;prompt_template = PromptTemplate(&#10;    input_variables=[&quot;long_term&quot;, &quot;short_term&quot;, &quot;query&quot;],&#10;    template=(&#10;        &quot;You are a Discord chat bot. You are given some context before the user's querry. Only take it into consideration if it makes sense.  \n&quot;&#10;        &quot;Relevant long-term context:\n{long_term}\n\n&quot;&#10;        &quot;Recent short-term conversation:\n{short_term}\n\n&quot;&#10;        &quot;Current user query:\n{query}\n&quot;&#10;        &quot;Respond to the user using all available context.&quot;&#10;    )&#10;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/bot/search.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/bot/search.py" />
              <option name="originalContent" value="from duckduckgo_search import DDGS&#10;&#10;def duckduckgo_search(query, max_results=5):&#10;    results = []&#10;    with DDGS() as ddgs:&#10;        for r in ddgs.text(query, max_results=max_results):&#10;            results.append(r.get(&quot;body&quot;, &quot;&quot;) or r.get(&quot;title&quot;, &quot;&quot;))&#10;    return &quot;\n&quot;.join(results) if results else &quot;No relevant results found.&quot;&#10;" />
              <option name="updatedContent" value="from duckduckgo_search import DDGS&#10;&#10;def duckduckgo_search(query, max_results=5):&#10;    results = []&#10;    with DDGS() as ddgs:&#10;        for r in ddgs.text(query, max_results=max_results):&#10;            results.append(r.get(&quot;body&quot;, &quot;&quot;) or r.get(&quot;title&quot;, &quot;&quot;))&#10;    return &quot;\n&quot;.join(results) if results else &quot;No relevant results found.&quot;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>